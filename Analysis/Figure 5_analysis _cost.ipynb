{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\"],\n",
    "        \"axes.labelsize\": 14,  # Match your ECAI paper font size\n",
    "        \"font.size\": 14,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"TabRepo\"\n",
    "\n",
    "dataset = pd.read_csv(\"../Datasets/\" + dataset_name + \".csv\")\n",
    "\n",
    "instances = sorted(dataset[\"instance\"].unique())\n",
    "print(instances)\n",
    "all_arm_index_list = dataset[\"arm_index\"].unique()\n",
    "valid_arm_index_list = [item for item in all_arm_index_list if item >= 0]\n",
    "number_of_arms = len(valid_arm_index_list)\n",
    "number_of_trails = len(dataset[\"repetition\"].unique())\n",
    "max_horizon_time = len(dataset[\"iteration\"].unique())\n",
    "combined_search_algorithms = dataset[dataset[\"arm_index\"] < 0][\"optimizer\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "def make_darker(color_rgba, darken_factor=0.8):\n",
    "    dark_color = tuple(\n",
    "        [max(0, c * darken_factor) for c in color_rgba[:3]] + [color_rgba[3]]\n",
    "    )\n",
    "    return dark_color\n",
    "\n",
    "\n",
    "isinstances = [\"Australian\"]\n",
    "instance = instances[12]\n",
    "print(instance)\n",
    "df = dataset[(dataset[\"instance\"] == instance)][\n",
    "    [\n",
    "        \"instance\",\n",
    "        \"arm_name\",\n",
    "        \"arm_index\",\n",
    "        \"repetition\",\n",
    "        \"iteration\",\n",
    "        \"loss\",\n",
    "        \"eval_time\",\n",
    "    ]\n",
    "]\n",
    "# Filter out rows with arm_index < 0\n",
    "df = df[df[\"arm_index\"] >= 0]\n",
    "df = df[(df[\"arm_index\"] == 0) | (df[\"arm_index\"] == 3)]\n",
    "# Count number of repetitions per (instance, arm_name, arm_index)\n",
    "rep_counts = df.groupby([\"instance\", \"arm_name\", \"iteration\"])[\"repetition\"].nunique()\n",
    "# Get only the groups with exactly 4 repetitions\n",
    "valid_iter_groups = rep_counts[rep_counts == 32].index\n",
    "filtered_df = (\n",
    "    df.set_index([\"instance\", \"arm_name\", \"iteration\"])\n",
    "    .loc[valid_iter_groups]\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "filtered_df[\"incumbent_loss\"] = (\n",
    "    filtered_df.sort_values([\"instance\", \"arm_index\", \"repetition\", \"iteration\"])\n",
    "    .groupby([\"instance\", \"arm_index\", \"repetition\"])[\"loss\"]\n",
    "    .cummin()\n",
    ")\n",
    "\n",
    "# First, get max iteration per (instance, arm_index)\n",
    "filtered_df[\"max_iteration\"] = filtered_df.groupby(\n",
    "    [\"instance\", \"arm_index\", \"repetition\"]\n",
    ")[\"iteration\"].transform(\"max\")\n",
    "\n",
    "# Compute iteration ratio\n",
    "filtered_df[\"iteration_ratio\"] = (filtered_df[\"iteration\"] + 1) / filtered_df[\n",
    "    \"max_iteration\"\n",
    "]\n",
    "filtered_df[\"avg_eval_time\"] = filtered_df.groupby(\n",
    "    [\"instance\", \"arm_index\", \"iteration\"]\n",
    ")[\"eval_time\"].transform(\"mean\")\n",
    "\n",
    "# Compute total eval time per (instance, arm_index)\n",
    "total_eval_time = (\n",
    "    filtered_df.groupby([\"instance\", \"arm_index\", \"repetition\"])[\n",
    "        \"avg_eval_time\"\n",
    "    ].transform(\"sum\")\n",
    "    # / number_of_trails\n",
    ")\n",
    "# print(total_eval_time.min())\n",
    "\n",
    "# Scale it by iteration ratio\n",
    "filtered_df[\"cum_eval_time\"] = (total_eval_time) * filtered_df[\"iteration_ratio\"]\n",
    "\n",
    "filtered_df[\"performance\"] = 1 - filtered_df[\"incumbent_loss\"]\n",
    "\n",
    "filtered_df = filtered_df.sort_values(\n",
    "    [\"instance\", \"arm_index\", \"repetition\", \"iteration\"]\n",
    ")\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "# Plot using seaborn with confidence interval\n",
    "plt.figure(figsize=(4, 2.5))\n",
    "plt.rcParams.update({\"font.size\": 10})\n",
    "ax = sns.lineplot(\n",
    "    data=filtered_df,\n",
    "    x=\"cum_eval_time\",\n",
    "    y=\"performance\",\n",
    "    hue=\"arm_name\",  # \"arm_name\",  # or 'arm_name' if you prefer\n",
    "    estimator=\"mean\",\n",
    "    errorbar=\"sd\",\n",
    "    err_kws={\"alpha\": 0.05},  # set error bar transparency here\n",
    "    linewidth=2.5,\n",
    "    markers=False,\n",
    ")\n",
    "\n",
    "# grouped = (\n",
    "#     filtered_df.groupby([\"arm_name\", \"arm_index\", \"cum_eval_time\"])[\"performance\"]\n",
    "#     .mean()\n",
    "#     .reset_index()\n",
    "# )\n",
    "# for i, (_, row) in enumerate(grouped.iterrows()):\n",
    "#     ax.plot(\n",
    "#         row[\"cum_eval_time\"],\n",
    "#         row[\"performance\"],\n",
    "#         marker=\"o\",\n",
    "#         markersize=5,  # change marker size here\n",
    "#         label=None,\n",
    "#         color=\"black\",  # optional: match color\n",
    "#     )\n",
    "\n",
    "mean_df = (\n",
    "    filtered_df.groupby([\"arm_name\", \"arm_index\", \"cum_eval_time\", \"iteration\"])\n",
    "    .performance.mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter only every 10th iteration\n",
    "every_10_df = mean_df[\n",
    "    (mean_df[\"iteration\"] % 10 == 0) & (mean_df[\"cum_eval_time\"] < 15 * 60)\n",
    "]\n",
    "\n",
    "# Step 3: Plot big markers\n",
    "for i, (_, row) in enumerate(every_10_df.iterrows()):\n",
    "    ax.plot(\n",
    "        row[\"cum_eval_time\"],\n",
    "        row[\"performance\"],\n",
    "        marker=\"o\",\n",
    "        markersize=8,  # adjust marker size here\n",
    "        color=ax.get_lines()[\n",
    "            row[\"arm_index\"]\n",
    "        ].get_color(),  # use the same color as the line\n",
    "        alpha=0.5,\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "\n",
    "    # Optional: add number next to marker\n",
    "    ax.text(\n",
    "        row[\"cum_eval_time\"] - 5 * (row[\"arm_index\"] - 3),\n",
    "        row[\"performance\"] + (0.01 * (row[\"arm_index\"] -1.5)*((row[\"cum_eval_time\"]>300)-0.5 ))-0.002,  # slight offset\n",
    "        f\"{row['iteration']}\",\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        color=make_darker(mcolors.to_rgba(\n",
    "            ax.get_lines()[row[\"arm_index\"]].get_color(), alpha=1.0\n",
    "        )),\n",
    "    )\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "new_labels = [\"CatBoost\", \"MLP\"]\n",
    "ax.legend(handles=handles, labels=new_labels, title=\"ML model\", loc=\"lower right\")\n",
    "# ax.get_legend().remove()\n",
    "\n",
    "# plt.title(\"OpenML-\" + str(instance))\n",
    "plt.xlabel(\"Time\")\n",
    "# Set x-axis ticks every hour (3600 seconds)\n",
    "xticks = np.arange(0, 3600 + 1, 5 * 60)\n",
    "xtick_labels = [f\"{int(t // 60)}m\" for t in xticks]\n",
    "\n",
    "plt.xticks(xticks, xtick_labels)\n",
    "\n",
    "plt.xlim(0, 15 * 60)  # change to your desired range\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"./figures/\" + dataset_name + \"_\" + str(instance) + \"_performance.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_instances = [\n",
    "    \"GAMETES_Heterogeneity_20atts_1600_Het_0_4_0_2_50_EDM-2_001\"\n",
    "]  # [ins for ins in instances if ins not in TabRepoRaw_instances]\n",
    "df = dataset[(dataset[\"arm_index\"] >= 0)]\n",
    "filtered_df = df[df[\"instance\"].isin(selected_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"loss_norm\"] = filtered_df.groupby(\"instance\")[\"loss\"].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "df = filtered_df[(filtered_df[\"arm_index\"] == 0) | (filtered_df[\"arm_index\"] == 3)]\n",
    "\n",
    "df[\"perforamnce\"] = 1 - df[\"loss\"]\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(2.5, 2.5))\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"eval_time\",\n",
    "    y=\"perforamnce\",\n",
    "    hue=\"arm_name\",\n",
    "    fill=True,\n",
    "    common_norm=False,  # Avoid normalizing across arms\n",
    "    alpha=0.5,  # Transparency for overlapping densities\n",
    "    levels=5,  # More contour levels\n",
    "    thresh=0.1,  # Avoid plotting very low-density areas\n",
    "    bw_adjust=5.0,\n",
    ")\n",
    "\n",
    "ax.legend_.remove()  # Remove the default legend\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel(\"Cost per iteration (s)\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"./figures/\" + dataset_name + \"_kde_plot.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
