{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def get_data(dataset_name, arm):\n",
    "    dataset = pd.read_csv(\"../../Datasets/\" + dataset_name + \".csv\")\n",
    "    instances = sorted(dataset[\"instance\"].unique())\n",
    "    all_arm_index_list = dataset[\"arm_index\"].unique()\n",
    "    valid_arm_index_list = [item for item in all_arm_index_list if item >= 0]\n",
    "    number_of_arms = len(valid_arm_index_list)\n",
    "    number_of_trails = len(dataset[\"repetition\"].unique())\n",
    "    horizon_time = len(dataset[\"iteration\"].unique())\n",
    "\n",
    "    d_stats = (\n",
    "        dataset[dataset[\"iteration\"] == 1]\n",
    "        .groupby([\"instance\", \"repetition\"])[\"loss\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "    )\n",
    "    dataset = dataset.merge(\n",
    "        d_stats, on=[\"instance\", \"repetition\"], suffixes=(\"\", \"_stats\")\n",
    "    )\n",
    "    dataset[\"norm_loss\"] = 1 / (\n",
    "        1 + np.exp(-(dataset[\"loss\"] - dataset[\"mean\"]) / (dataset[\"std\"] + 0.01))\n",
    "    )\n",
    "    dataset = dataset.drop(columns=[\"mean\", \"std\"])\n",
    "    # def min_max_norm(x):\n",
    "    #     print(x)\n",
    "    #     return 1 / (1 + np.exp(-(x - np.mean(d)) / (np.std(d) + 0.01)))\n",
    "\n",
    "    # dataset[\"norm_loss\"] = dataset.groupby(\"instance\")[\"loss\"].transform(min_max_norm)\n",
    "\n",
    "    df = dataset[(dataset[\"arm_index\"] >= 0) & (dataset[\"arm_index\"] == arm)]\n",
    "    df = df[[\"instance\", \"arm_index\", \"repetition\", \"iteration\", \"norm_loss\"]]\n",
    "    real_data = df.sort_values(by=[\"instance\", \"arm_index\", \"repetition\", \"iteration\"])[\n",
    "        \"norm_loss\"\n",
    "    ].values.reshape(len(instances), 1, number_of_trails, horizon_time)\n",
    "    return real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "\n",
    "def non_stationary_skew_normal_dist(sigma1, sigma2, seq_len):\n",
    "    skewness = np.random.uniform(-100.0, -20.0)\n",
    "    loc1 = np.random.uniform(0.0, 1.0)\n",
    "    scale1 = np.random.uniform(0.0, sigma1)\n",
    "\n",
    "    quantile1 = skewnorm.cdf(0.0, a=skewness, loc=loc1, scale=scale1)\n",
    "    quantile2 = skewnorm.cdf(1.0, a=skewness, loc=loc1, scale=scale1)\n",
    "    samples_1 = skewnorm.ppf(\n",
    "        np.random.uniform(quantile1, quantile2, size=seq_len),\n",
    "        a=skewness,\n",
    "        loc=loc1,\n",
    "        scale=scale1,\n",
    "    )\n",
    "\n",
    "    skewness = np.random.uniform(-100.0, -20.0)\n",
    "    scale2 = np.random.uniform(sigma2 * (1 - loc1), sigma2)\n",
    "    quantile1 = skewnorm.cdf(0.0, a=skewness, loc=1.0, scale=scale2)\n",
    "    quantile2 = skewnorm.cdf(1.0, a=skewness, loc=1.0, scale=scale2)\n",
    "    samples_3 = skewnorm.ppf(\n",
    "        np.random.uniform(quantile1, quantile2, size=seq_len),\n",
    "        a=skewness,\n",
    "        loc=1.0,\n",
    "        scale=scale2,\n",
    "    )\n",
    "    samples_3 = np.sort(samples_3)\n",
    "    mixed_samples = samples_1 * samples_3\n",
    "    mixed_samples[mixed_samples > 1.0] = 1.0\n",
    "    mixed_samples[mixed_samples < 0.0] = 0.0\n",
    "    return mixed_samples\n",
    "\n",
    "\n",
    "def get_batch_func(\n",
    "    batch_size,\n",
    "    seq_len,\n",
    "    num_features=1,\n",
    "    device=\"cpu\",\n",
    "    hyperparameters=None,\n",
    "    noisy_target=True,\n",
    "    num_outputs=1,\n",
    "    sigma1=0.1,\n",
    "    sigma2=0.1,\n",
    "    to_torch=False,\n",
    "    **_,\n",
    "):\n",
    "    xs = np.zeros((seq_len, batch_size, num_features))\n",
    "    ys = np.zeros((seq_len, batch_size, num_outputs))\n",
    "    ys_noisy = np.zeros((seq_len, batch_size, num_outputs))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        xs[:, i, 0] = np.arange(1, seq_len + 1)\n",
    "        data = non_stationary_skew_normal_dist(sigma1, sigma2, seq_len)\n",
    "        ys[:, i, 0] = np.maximum.accumulate(data, axis=0)\n",
    "        ys_noisy[:, i, 0] = np.maximum.accumulate(\n",
    "            data + np.random.normal(0.0, 0.001, seq_len), axis=0\n",
    "        )\n",
    "        if num_outputs > 1:\n",
    "            ys[:, i, 1] = data\n",
    "    if to_torch:\n",
    "        xs = torch.from_numpy(xs.astype(np.float32))\n",
    "        ys = torch.from_numpy(ys.astype(np.float32))\n",
    "        ys_noisy = torch.from_numpy(ys_noisy.astype(np.float32))\n",
    "        return xs.to(device), ys.to(device), ys.to(device)\n",
    "    else:\n",
    "        return xs, ys, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def error_from_trends(A_data, B_data, ci=0.95):\n",
    "    print(\"Calculating error from trends...\", A_data.shape , B_data.shape)\n",
    "    mean_A = A_data.mean(axis=0)\n",
    "    mean_B = B_data.mean(axis=0)\n",
    "    return np.sqrt(np.mean((mean_A - mean_B)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def plot_real_vs_synthetic(\n",
    "    dataset_name, T, arm_names, arm, sigma1, sigma2, samples=1000\n",
    "):\n",
    "    real_datas = []\n",
    "    real_datas.append(get_data(dataset_name, arm))\n",
    "\n",
    "    real_data = []\n",
    "    for d in real_datas:\n",
    "        arr = d.copy().reshape(\n",
    "            d.shape[0] * d.shape[1] * d.shape[2],\n",
    "            d.shape[3],\n",
    "        )\n",
    "        real_data.append(arr)\n",
    "    real_data = np.concatenate(real_data)\n",
    "\n",
    "    # Assuming `data` is your (1200, 250) array\n",
    "    first_observations = 200\n",
    "    column_0 = np.max(real_data[:, 0:first_observations], axis=1)  # Mean across the first dimension\n",
    "    # Create 20 bins\n",
    "    num_bins = 10\n",
    "    bins = np.linspace(0, 1, num_bins + 1)\n",
    "\n",
    "    grouped_data = {i: [] for i in range(num_bins)}\n",
    "    bin_indices = np.digitize(column_0, bins) - 1\n",
    "    for i, bin_idx in enumerate(bin_indices):\n",
    "        if 0 <= bin_idx < num_bins:\n",
    "            grouped_data[bin_idx].append(real_data[i])\n",
    "\n",
    "    # Optional: Convert lists to arrays\n",
    "    grouped_data_real_data = {\n",
    "        k: np.array(v) for k, v in grouped_data.items() if len(v) > 0\n",
    "    }\n",
    "    grouped_data_synthetic_data = {}\n",
    "    num_sample = samples\n",
    "    _, synthetic_data, _ = get_batch_func(\n",
    "        num_sample, T, num_outputs=2, sigma1=sigma1, sigma2=sigma2\n",
    "    )\n",
    "    synthetic_series = synthetic_data[:, :, 1].T\n",
    "\n",
    "    column_0 = np.max(synthetic_series[:, 0:first_observations], axis=1)\n",
    "    bin_indices = np.digitize(column_0, bins) - 1\n",
    "    grouped_data = {i: [] for i in range(num_bins)}\n",
    "    for i, bin_idx in enumerate(bin_indices):\n",
    "        if 0 <= bin_idx < num_bins:\n",
    "            grouped_data[bin_idx].append(synthetic_series[i])\n",
    "\n",
    "    # Optional: Convert lists to arrays\n",
    "    grouped_data_synthetic_data = {\n",
    "        k: np.array(v) for k, v in grouped_data.items() if len(v) > 0\n",
    "    }\n",
    "    all_a = []\n",
    "    all_b = []\n",
    "    for i in grouped_data_real_data.keys():\n",
    "        if i not in grouped_data_synthetic_data.keys():\n",
    "            continue\n",
    "        start_bin = bins[i] \n",
    "        b_min = 0\n",
    "        b_max =  bins[i+1]\n",
    "        #max(grouped_data_real_data[i].max(axis=0).mean(), grouped_data_synthetic_data[i].max(axis=0).mean())\n",
    "        a = ( np.maximum.accumulate(grouped_data_synthetic_data[i], axis=1) - b_min)/ ( b_max)\n",
    "        b = (np.maximum.accumulate(grouped_data_real_data[i], axis=1) - b_min ) / ( b_max)\n",
    "        all_a.append(a)\n",
    "        all_b.append(b)\n",
    "\n",
    "    all_a = np.concatenate(all_a, axis=0)\n",
    "    all_b = np.concatenate(all_b, axis=0)\n",
    "\n",
    "    def get_median_and_ci(matrix, lower=25, upper=75):\n",
    "        median = np.median(matrix, axis=0)\n",
    "        lower_ci = np.percentile(matrix, lower, axis=0)\n",
    "        upper_ci = np.percentile(matrix, upper, axis=0)\n",
    "        return median, lower_ci, upper_ci\n",
    "\n",
    "    # Group A\n",
    "    median_a, low_a, high_a = get_median_and_ci(all_a)\n",
    "\n",
    "    # Group B\n",
    "    median_b, low_b, high_b = get_median_and_ci(all_b)\n",
    "\n",
    "    error = error_from_trends(all_a, all_b)\n",
    "\n",
    "    time = np.arange(T)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.rcParams.update({\"font.size\": 12})\n",
    "\n",
    "    # Plot group A with shaded std\n",
    "    plt.plot(\n",
    "        time,\n",
    "        median_a,\n",
    "        color=\"blue\",\n",
    "        label=\"Synth($\\\\sigma_1=\" + str(sigma1) + \", \\\\sigma_2=\" + str(sigma2) + \"$)\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    plt.fill_between(time, low_a, high_a, color=\"blue\", alpha=0.2)\n",
    "\n",
    "    # Plot group B with shaded std\n",
    "    plt.plot(\n",
    "        time,\n",
    "        median_b,\n",
    "        color=\"orange\",\n",
    "        label= dataset_name.split(\"_\")[0] + \"-\" + arm_names[arm],  # Real(\" + dataset_name.split(\"_\")[0] + \"-\" + arm_names[arm] + \")\",\n",
    "        linewidth=3,\n",
    "    )\n",
    "    plt.fill_between(time, low_b, high_b, color=\"orange\", alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Scaled performance\")\n",
    "    plt.legend(title=f\"RMSE = {error:.3f}\", loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    path = \"./figures/paper_prior_selection_\" + dataset_name + \"/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    plt.savefig(\n",
    "        path + arm_names[arm] + \"_\" + str(sigma1) + \"_\" + str(sigma2) + \".pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Reshuffling\"\n",
    "dataset_name = \"YaHPOGym_3\"\n",
    "#dataset_name = \"TabRepoRaw_8\"\n",
    "\n",
    "max_retries = 5\n",
    "T = 200\n",
    "if dataset_name == \"Reshuffling\":\n",
    "    arm_names = [\"MLP\", \"Logreg\", \"CatBoost\", \"XGBoost\"]\n",
    "    T = 250\n",
    "if dataset_name == \"YaHPOGym_3\":\n",
    "    arm_names = [\"AKNN\", \"GLMNet\", \"Ranger\", \"RPart\", \"SVM\", \"XGBoost\"]\n",
    "if dataset_name == \"TabRepoRaw_8\":\n",
    "    arm_names = [\n",
    "        \"CatBoost\",\n",
    "        \"ExtraTrees\",\n",
    "        \"LightGBM\",\n",
    "        \"NeuralNet(FastAI)\",\n",
    "        \"NeuralNet(Torch)\",\n",
    "        \"RandomForest\",\n",
    "        \"XGBoost\",\n",
    "    ]\n",
    "\n",
    "sigma1_sigma2 = [(0.1, 0.001), (0.2, 0.01), (0.2, 0.1)]\n",
    "\n",
    "for sigma1, sigma2 in sigma1_sigma2:\n",
    "    for arm in range(len(arm_names)):\n",
    "        print(\"arm_name\", arm, arm_names[arm], \"sigma\", sigma1, sigma2)\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Your code here (e.g., network request, file read, etc.)\n",
    "                result = plot_real_vs_synthetic(\n",
    "                    dataset_name, T, arm_names, arm, sigma1, sigma2\n",
    "                )\n",
    "                break  # Success, exit the loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"Max retries reached. Exiting.\")\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
